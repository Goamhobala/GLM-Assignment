---
title: "STA304XS - Assignment 2: Machine Learning"
author:
  - name: Jing Yeh
    email: yhxjin001@myuct.ac.za
  - name: Saurav Sathnarayan
    email: sthsau001@myuct.ac.za
date: "`r Sys.Date()`"
output: 
  rticles::oup_article:
    oup_version: 0
    extra_dependencies: booktabs
abstract: |

keywords: >

---

```{r setup, include=FALSE}
# Global options for the document
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.pos = '!ht', # =- Change this line
  out.width = '100%', 
  dpi = 300
)

# Load all required packages
library(knitr)
library(kableExtra)
library(glmnet)
```


```{r echo=FALSE, out.width="400px", fig.align='center'}

```
\newpage
\tableofcontents
\newpage

## Question 1 - BAYESIAN INTERPRETATION

We start with the logistic regression model:

$$\Pr(Y_i = 1 \mid x_i) = \text{logit}^{-1}(x_i^\top \beta) = \dfrac{1}{1 + \exp(-x_i^\top \beta)}.$$

The log-likelihood for all $n$ observations is

$$l(\beta) = \sum_{i=1}^n \left[ y_i x_i^\top \beta - \log(1 + e^{x_i^\top \beta}) \right].$$

Assume independent Normal priors for the coefficients:

$$\beta_j \sim N(0, \tau^2), \quad j = 1, \ldots, p.$$

Hence, the prior density is

$$\pi(\beta) = \prod_{j=1}^p \dfrac{1}{\sqrt{2\pi\tau^2}}
\exp\!\left(-\dfrac{\beta_j^2}{2\tau^2}\right)$$

Taking logs, we obtain the log-prior:

$$\log \pi(\beta) = -\dfrac{p}{2}\log(2\pi\tau^2)
-\dfrac{1}{2\tau^2}\sum_{j=1}^p \beta_j^2$$

Now, by Bayes' theorem,

$$\pi(\beta \mid y) = \dfrac{\pi(y \mid \beta)\, \pi(\beta)}{\pi(y)}$$

Taking logs of both sides gives

$$\log \pi(\beta \mid y) = \log \pi(y \mid \beta) + \log \pi(\beta) - \log \pi(y)$$

The term $\log \pi(y)$ is a normalising constant that does not depend on $\beta$,
so when maximising over $\beta$, it can be ignored. Therefore,

$$\log \pi(\beta \mid y) \propto \log \pi(y \mid \beta) + \log \pi(\beta)$$

Substituting the expressions for $\log p(y \mid \beta)$ and $\log p(\beta)$, we have

$$\log p(\beta \mid y) \propto l(\beta) - \dfrac{1}{2\tau^2}\sum_{j=1}^p \beta_j^2$$

This is the expression for the log-posterior up to a constant.

To obtain the maximum a posteriori (MAP) estimate,
we maximise $\log p(\beta \mid y)$ with respect to $\beta$.
Equivalently, we minimise the negative log-posterior:

$$\widehat{\beta}_{MAP} = \arg\min_\beta
\left[ -l(\beta) + \dfrac{1}{2\tau^2}\sum_{j=1}^p \beta_j^2 \right]$$

If we define $\lambda = \dfrac{1}{2\tau^2}$,
then the optimisation problem becomes

$$\boxed{
\widehat{\beta}_{MAP}
= \arg\min_\beta \left[ -l(\beta) + \lambda \|\beta\|_2^2 \right]
}$$

This shows that the MAP estimator under a Normal prior
is equivalent to the ridge-regularised logistic regression estimator,
where the penalty parameter $\lambda$ corresponds to the precision of the prior.

(b)


```{r eval=FALSE, include=FALSE}
'**Role of lambda$ or $\tau^2$:**
  - lambda$ controls the amount of shrinkage or regularisation applied to the coefficients.
  - A **large lambda$** (small $\tau^2$) implies a strong prior belief that coefficients should be close to zero, resulting in **greater shrinkage**.
  - A **small lambda$** (large $\tau^2$) implies a weak prior, approaching the unpenalised **maximum likelihood estimate (MLE)**.

- **Effect of this regularisation:**
  - This corresponds to **ridge regression** (L2 regularisation) in the frequentist framework.
  - It helps prevent **overfitting** by penalising large coefficient values.


- **Bayesian vs. MLE perspective:**
  - The MLE only uses the data and can overfit when the sample size is small.
  - The Bayesian (MAP) approach incorporates **prior information** through $\tau^2$, providing a **probabilistic justification** for regularisation.
  - Hence, the Bayesian view explains **why** regularisation arises naturally as a consequence of imposing a Gaussian prior.'
```

## Question 2 - DERIVING RIDGE-IWLS

(a)

In IWLS, the weight for observation $i$ is given by
$$
w_i^{(t)} = \frac{1}{\operatorname{Var}(Y_i)} \left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2.
$$

For logistic regression, $\operatorname{Var}(Y_i) = p_i (1 - p_i)$ and 
$\frac{\partial \mu_i}{\partial \eta_i} = p_i (1 - p_i)$, so

$$
w_i^{(t)} = \frac{\left(p_i^{(t)} (1 - p_i^{(t)})\right)^2}{p_i^{(t)} (1 - p_i^{(t)})} 
= p_i^{(t)} (1 - p_i^{(t)}).
$$

These are exactly the diagonal entries of the weight matrix:

$$
W^{(t)} = \operatorname{diag}\Big(p_1^{(t)} (1 - p_1^{(t)}), \dots, p_n^{(t)} (1 - p_n^{(t)})\Big).
$$

and $p^{(t)} = (p_1^{(t)}, \dots, p_n^{(t)})^\top$ are the predicted probabilities at $\beta^{(t)}$.

and for $z^{(t)}$


$$z_i^{(t)} = \eta_i^{(t)} + \frac{y_i - \mu_i^{(t)}}{\frac{\partial \mu_i}{\partial \eta_i}}$$

where 
$$
\eta_i^{(t)} = x_i^\top \beta^{(t)}, \quad \mu_i^{(t)} = \sigma(\eta_i^{(t)}) = p_i^{(t)}, \quad \frac{\partial \mu_i}{\partial \eta_i} = p_i^{(t)} (1 - p_i^{(t)}).
$$

Substituting the derivative for logistic regression gives

$$
z_i^{(t)} = x_i^\top \beta^{(t)} + \frac{y_i - p_i^{(t)}}{p_i^{(t)} (1 - p_i^{(t)})}.
$$

In matrix form, for all $n$ observations:

$$
z^{(t)} = X \beta^{(t)} + (W^{(t)})^{-1} (y - p^{(t)}),
$$

From our IWLS formula, we have:

$$ (\textbf{X}^T \textbf{W}^{(t)} \textbf{X})^{-1} \beta^{(t+1)} = \textbf{X}^T \textbf{W}^{(t)} z^{(t)} $$
Thus, 

$$\boxed{\beta^{(t+1)} = (\textbf{X}^T \textbf{W}^{(t)} \textbf{X})^{-1}\textbf{X}^T \textbf{W}^{(t)} z^{(t)}}   $$
(b)
\newpage

## Question 3 - IMPLEMENTATION AND EVALUATION
(a)

```{r include=FALSE}
dat = read.table('~/Desktop/template/Assignment_Disease.txt', header = TRUE)
attach(dat)
Y = dat$DiseaseStatus
X = as.matrix(cbind(1, dat[, c("Measurement1", "Measurement2", "Measurement3")]))
n = length(Y)
p = ncol(X)
```

```{r echo=TRUE}
glm_fit = function(Y, X, lambda = 1, eps = 1e-6, K = 100) {
  beta = rep(0, p)
  for (iter in 1:K) {
    
    # Linear predictor
    eta = as.vector(X %*% beta)
    
    # For logistic regression (binomial), use:
    mu = 1 / (1 + exp(-eta))
    # Weights and working response
    
    W = diag(as.vector(mu * (1 - mu)), n, n)
    z = eta + (Y - mu) / (mu * (1 - mu))
    
    # Lambda Scaling
    P = diag(rep(lambda / n, p))
    
    # Ridge-IWLS updated
    XtWX = t(X) %*% W %*% X
    XtWz = t(X) %*% W %*% z
    beta_new = solve(XtWX + 2 * lambda * diag(p), XtWz)
    
    # Check convergence
    if (sqrt(sum((beta_new - beta)^2)) < eps) break
    beta = beta_new
  }
  return = list(coefficients = beta, iterations = iter, converged = (iter < K))
}
```

```{r include=FALSE}
fit = glm_fit(Y, X, lambda = 1)
fit_glmnet = glmnet(X[,-1], Y, family = "binomial", alpha = 0, lambda = 1/n, intercept = TRUE, standardize = FALSE )
```

(b) & (c)

### Estimates
Looking at our estimates, we see that our intercepts are very different from each other. This is due to the fact that,in IWLS, our X matrix includes a column of ones and no centering/standardization is done. However, the glmnet package fits an intercept separately and standardises for us, even with standardize = FALSE, the intercept optimization is treated separately. Other differences in the magnitudes of coefficients are small. 

```{r}
#Ridge IWLS coefficients
df <- data.frame(
  Estimate = fit$coefficients
)
kable(df, format = "latex", booktabs = TRUE, caption = "Ridge IWLS - Coefficient estimates.") %>%
  kable_styling(latex_options = "hold_position")

# glmnet coefficients
coefs <- as.matrix(coef(fit_glmnet))
df2 <- data.frame(
  Term = rownames(coefs),
  Estimate = coefs[, 1]
)
kable(df2, format = "latex", booktabs = TRUE, caption = "Glmnet coefficient estimates.") %>%
  kable_styling(latex_options = "hold_position")
```

### Prediction Performance


```{r}

# IWLS predictions
eta_iwls = X %*% fit$coefficients
p_iwls = 1 / (1 + exp(-eta_iwls))


# glmnet predictions
p_glmnet = predict(fit_glmnet,X[,-1], type = "response", s = 1/n)

#mean squared error
mse_iwls = mean((Y - p_iwls)^2)
mse_glmnet = mean((Y - p_glmnet)^2)


pred_df = data.frame(
  Obs = 1:10,
  Actual = Y[1:10],
  IWLS_Pred = round(p_iwls[1:10], 4),
  Glmnet_Pred = round(p_glmnet[1:10], 4)
)

# Display with kable
kable(pred_df, format = "latex", booktabs = TRUE, caption = "Predicted probabilities: Ridge-IWLS vs. glmnet") %>%
  kable_styling(latex_options = "hold_position", position = "center")


perf <- data.frame(
  Model = c("Ridge-IWLS", "glmnet"),
  MSE = c(round(mse_iwls, 6), round(mse_glmnet, 6)))
  kable(perf, format = "latex", booktabs = TRUE, caption = "Model performance comparison (MSE)") %>%
  kable_styling(latex_options = "hold_position", position = "center")
```


Despite different intercepts, the predicted probabilities are be similar.
Small differences in slopes and intercept can slightly shift the probabilities, but classification accuracy or nearly identical. In our tables of predicted probalities we have an 'Actual' Column which tells us the disease status of our observations.




### Algorithmic Stability and Limitations of IWLS
IWLS is slower for large p or n, because each iteration requires computing t(X) %*% W %*% X. While, glmnet uses coordinate descent, which is much faster and more memory-efficient for large datasets. IWLS is good small problems, but for large datasets, glmnet is preferable. Differences in intercept values are normal and expected due to centering, scaling, and optimization differences. With proper scaling and intercept handling, IWLS can approximate glmnet closely, especially for slope coefficients.

## QUESTION 4 - BONUS: VISUALISATION AND ANALYSIS

```{r}
fit = glmnet(X[,-1], Y, family = "binomial",alpha = 0, intercept = TRUE, standardize = FALSE)
plot(fit, xvar = "lambda", label = TRUE, main = 'Coeffcient shrinkage as Lambda increases')
```

A smaller -log($\lambda$) value indicates a larger penalty, therefore our coefficients reduce to zero, as -log($\lambda$) increases, our coefficients move to their true values.
```{r}

set.seed(2025)

# Split into training and validation sets
train_idx = sample(1:n, size = 0.7*n)
X_train = X[train_idx, ]
Y_train = Y[train_idx]
X_val = X[-train_idx, ]
Y_val = Y[-train_idx]

# Fit Ridge (alpha = 0) over a sequence of lambda values
fit = glmnet(X_train, Y_train, family = "binomial", alpha = 0, standardize = TRUE)

# Lambda sequence
lambda_seq = fit$lambda
n_lambda = length(lambda_seq)

# Initialize accuracy vectors
train_acc = numeric(n_lambda)
val_acc = numeric(n_lambda)

# Compute training and validation accuracy for each lambda
for (i in 1:n_lambda) {
  pred_train = predict(fit, newx = X_train, s = lambda_seq[i], type = "response")
  pred_val = predict(fit, newx = X_val, s = lambda_seq[i], type = "response")
  
  train_acc[i] = mean((pred_train > 0.5) == Y_train)
  val_acc[i] = mean((pred_val > 0.5) == Y_val)
}

# Plot
plot(-log(lambda_seq), train_acc, type = "l", col = "blue", lwd = 2,
     xlab = "-log(lambda)", ylab = "Accuracy", ylim = c(0.5, 1),
     main = "Training vs Validation Accuracy vs Lambda")
lines(-log(lambda_seq), val_acc, col = "red", lwd = 2)
legend("bottomright", legend = c("Training", "Validation"),
       col = c("blue", "red"), lwd = 2)

```

To evaluate the effect of the regularization parameter  $\lambda$ on model performance, we computed predicted probabilities from both the Ridge-IWLS model and glmnet logistic regression across a sequence of $\lambda$ values. For each $\lambda$, the predicted probabilities were converted into binary class predictions using a threshold of 0.5, and accuracy was calculated separately on the training and validation sets. This procedure allows us to observe how increasing $\lambda$ (stronger regularization) shrinks the coefficients, reducing variance and preventing overfitting. Typically, training accuracy decreases as $\lambda$ increases due to the penalty constraining the model, while validation accuracy initially rises to a peak at an optimal $\lambda$ before declining as over-regularization induces underfitting. This analysis illustrates the bias–variance tradeoff and demonstrates how ridge regularization improves generalization compared to unpenalized logistic regression.